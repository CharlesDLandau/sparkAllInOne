{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision Spark Clusters and Run Jobs Right in Your Notebook\n",
    "\n",
    "![Spark!](./img/jez-timms-spark.jpg)\n",
    "Photo by Jez Timms on [Unsplash](https://unsplash.com/photos/_Ch_onWf38o)\n",
    "\n",
    "We'll be using GCP SDKs to provision Spark clusters and run PySpark jobs on them -- all from a notebook. In fact, the source for this document IS a notebook. You can clone this demo here:\n",
    "\n",
    "[]()\n",
    "\n",
    "This will take between five and ten minutes. I'm so spoiled by the public cloud that I don't even really understand how crazy this level of convenience is.\n",
    "\n",
    "In this activity we will need a few prerequisites. They are:\n",
    "\n",
    "1. A project directory with a python virtual environment\n",
    "\n",
    "```\n",
    "mkdir sparkDemo && cd sparkDemo\n",
    "python -m virtualenv venv\n",
    "\n",
    "// windows\n",
    "call venv/Scripts/activate\n",
    "\n",
    "// else\n",
    "source venv/bin/activate\n",
    "\n",
    "pip install google-cloud-dataproc jupyterlab\n",
    "```\n",
    "\n",
    "2. A billing-enabled project on GCP.\n",
    "\n",
    "+ [Create a project](https://console.cloud.google.com/iam-admin/projects)\n",
    "\n",
    "+ [Enable billing](https://cloud.google.com/billing/docs/how-to/modify-project#enable_billing_for_a_project)\n",
    "\n",
    "+ Note the project ID. It's listed front and center on the [GCP home screen](https://console.cloud.google.com/home/) (make sure you have the right project open). Also feel free to edit in a nearby region from [this list](https://cloud.google.com/compute/docs/regions-zones/#locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'sparkdemo'\n",
    "region = 'us-east1-b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. A service account, and the JSON file with its credentials\n",
    "\n",
    "+ Go to the [IAM console](https://console.cloud.google.com/iam-admin) and go to Service Accounts (left menu). Create a new account and fill in the details sensibly. Give the service account `dataproc worker` permissions. Remember to save the credentials file as JSON when prompted. My habit is to save it as `client_secrets.json` and **then I immediately add that filename to my gitignore file.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With those in hand...lets set up our client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import dataproc_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dataproc_v1.ClusterControllerClient.from_service_account_json(\n",
    "    './client_secrets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we imported the `dataproc_v1` SDK, and we instantiated a client with the special `from_service_account_json` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A parser for google's responses\n",
    "\n",
    "def response_parser(client_obj, verbose=False):\n",
    "    responses = []\n",
    "    for page in client_obj.pages:\n",
    "        for element in page:\n",
    "            responses.append(element)\n",
    "           \n",
    "    if not responses and verbose:\n",
    "        print('No responses found!')\n",
    "        \n",
    "    if responses and verbose:\n",
    "        for resp in responses:\n",
    "            print(resp)\n",
    "    \n",
    "    return responses\n",
    "        \n",
    "clusters = client.list_clusters(project_id, 'global')\n",
    "responses = response_parser(clusters);\n",
    "len(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are showing zero clusters provisioned. Let's provision a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A response handler that we can attach to cluster actions\n",
    "\n",
    "def handler(ops):\n",
    "    result = ops.result()\n",
    "    \n",
    "    # Uncomment this to see results from the calls in this notebook\n",
    "    # (this can also help you learn config options)\n",
    "    # print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important thing here is to create a `dataproc_v1.types.Cluster` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'gce_cluster_config':{'zone_uri':region}}\n",
    "cluster_config = dataproc_v1.types.Cluster(\n",
    "    project_id=project_id, cluster_name=\"democluster\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCP uses sensible defaults to configure this instance, but you can use the printout from the callback handler to examine the config fields and override where appropriate. For example to scale up workers add the following to the `config`:\n",
    "\n",
    "    'worker_config':{'num_instances':3}\n",
    "\n",
    "Also note that the `region` is `global` at the call to `create_cluster`, and not in the `gce_cluster_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "creating = client.create_cluster(project_id, 'global', cluster_config)\n",
    "creating.add_done_callback(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you enabled printing in the callback, you can just wait for that to fire now. Once the cluster is up we can repeat our call to `list_clusters` and see that we have one now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = client.list_clusters(project_id, 'global')\n",
    "\n",
    "responses = response_parser(clusters);\n",
    "len(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! \n",
    "\n",
    "#### Run a job\n",
    "\n",
    "Now let's set up a job controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobber = dataproc_v1.JobControllerClient.from_service_account_json(\n",
    "    './client_secrets.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need somewhere to put our job files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import uuid\n",
    "\n",
    "gs = storage.Client.from_service_account_json('./client_secrets.json')\n",
    "bucket = gs.create_bucket(f'sparkdemo-{uuid.uuid1()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store our code in the bucket. If you have Spark installed, this would be the part where you could start prototyping in local mode. For now, just copy the following code into a file `main.py` and save it in the root of your project.\n",
    "\n",
    "```\n",
    "import pyspark\n",
    "import sys\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "# Dummy task\n",
    "terms = [\"fodder\" for x in range(9000)] + [\"spartan\", \"sparring\", \"sparrow\"]\n",
    "words = sc.parallelize(terms)\n",
    "words.filter(lambda w: w.startswith(\"spar\")).take(2)\n",
    "\n",
    "# Output target was passed to args config field\n",
    "bucket = sys.argv[1]\n",
    "output_directory = 'gs://{}/pyspark_output'.format(bucket)\n",
    "\n",
    "# For csv output\n",
    "sqlContext = pyspark.SQLContext(sc)\n",
    "df = sqlContext.createDataFrame(words, pyspark.sql.types.StringType())\n",
    "df.coalesce(1).write.format('com.databricks.spark.csv'\n",
    "            ).options(header='true').save(output_directory)\n",
    "```\n",
    "\n",
    "Here's a simple function that is barely modified from the boilerplate on the storage client library docs. We'll use it to upload files to our bucket, specifically our `main.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ./main.py uploaded to input_files/main.py.\n"
     ]
    }
   ],
   "source": [
    "def upload_blob(bucket, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print('File {} uploaded to {}.'.format(\n",
    "        source_file_name,\n",
    "        destination_blob_name))\n",
    "    \n",
    "# Use the helper function to upload our code\n",
    "upload_blob(bucket, './main.py', \"input_files/main.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our code is in place, we need to make an instance of `dataproc_v1.types.PySparkJob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_job = dataproc_v1.types.PySparkJob(\n",
    "    main_python_file_uri=f\"gs://{bucket.name}/input_files/main.py\",\n",
    "    args={bucket.name}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`main_python_file_uri` points to our code. We could also pass `python_file_uris` and `file_uris` if we had more supporting files to send, but we don't. \n",
    "\n",
    "Our `main.py` script uses `sys.argv[1]` to construct an output directory in our bucket, so we pass that to `args`.\n",
    "\n",
    "We also need to place the job in our cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "placement = dataproc_v1.types.JobPlacement(cluster_name=\"democluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to submit a job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = jobber.submit_job(project_id, 'global', {'placement': placement,\n",
    "                                               'pyspark_job':pyspark_job})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a minute or so the job finishes. We can check on it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state: RUNNING\n",
       "state_start_time {\n",
       "  seconds: 1563547231\n",
       "  nanos: 583000000\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_status = jobber.get_job(project_id, 'global', job.job_uuid)\n",
    "job_status.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state: DONE\n",
       "state_start_time {\n",
       "  seconds: 1563547265\n",
       "  nanos: 793000000\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_status = jobber.get_job(project_id, 'global', job.job_uuid)\n",
    "job_status.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jobs Done!\n",
    "\n",
    "![We did it!](https://media.giphy.com/media/3o84U9arAYRM73AIvu/giphy.gif)\n",
    "\n",
    "Let's download contents from our bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_files/main.py\n",
      "pyspark_output/\n",
      "pyspark_output/_SUCCESS\n",
      "pyspark_output/part-00000-df4302c1-aec3-415b-84d6-66bd915ae938-c000.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.mkdir('output')\n",
    "except FileExistsError as e:\n",
    "    pass\n",
    "\n",
    "for page in bucket.list_blobs().pages:\n",
    "    for element in page:\n",
    "        print(element.name)\n",
    "        blob = bucket.blob(element.name)\n",
    "        destination = element.name.split('/')[-1]\n",
    "        if destination:\n",
    "            blob.download_to_filename(f\"./output/{destination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our outputs are downloaded locally. We're done, so let's tear it all down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "destroying = client.delete_cluster(project_id, 'global', \"democluster\")\n",
    "destroying.add_done_callback(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and delete bucket\n",
    "for page in bucket.list_blobs().pages:\n",
    "    for element in page:\n",
    "        bucket.delete_blob(element.name)\n",
    "bucket.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it. Now spark clusters are a disposable thing for you. When you want one, you can spin it up at any size your credit card will support. Prototype your jobs locally and submit them from your notebook. When you're done, tear it back down.\n",
    "\n",
    "Why would you want to know how to do this? For one-off batch processing and reporting. When would you definitely not use this? In production.\n",
    "\n",
    "#### Reference Docs\n",
    "\n",
    "1. https://googleapis.github.io/google-cloud-python/latest/dataproc/\n",
    "2. https://googleapis.github.io/google-cloud-python/latest/storage/\n",
    "3. https://spark.apache.org/docs/2.1.0/api/python/\n",
    "\n",
    "#### Thank you!\n",
    "\n",
    "Thank you for reading. I hope this has been helpful for you.\n",
    "\n",
    "I'm always learning, so if you see anything wrong in here I hope you'll leave me a comment to share your insights. Or just leave a comment anyway.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
